{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvcxlXa1FzgeLxm2hn2vff"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installs\n",
        "\n",
        "We need **HuggingFace** & Weights & Biases."
      ],
      "metadata": {
        "id": "3xZvsQfg-0uP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtTx0zXI-pnu"
      },
      "outputs": [],
      "source": [
        "# Install HuggingFace\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Weights and Biases\n",
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "q_DZjac_AwH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tried this because .train wouldn't work\n",
        "# pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "6Nr-sq2XTzUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and login to WandB"
      ],
      "metadata": {
        "id": "TnVqkIwx_Fws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import wandb\n",
        "import wandb\n",
        "\n",
        "# Login with your authentication key - you'll need to have a wandb account to generate this\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "SCHqgens-y4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup wandb environment variables\n",
        "%env WANDB_ENTITY=mkoven\n",
        "%env WANDB_PROJECT=testproject"
      ],
      "metadata": {
        "id": "8mGxv7ytBAzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Processing using a Tokenizer with Hugging Face\n",
        "\n",
        "In NLP, tokenizing a text block involves splitting it into words or subwords, which then are converted to IDs through a look-up table.\n",
        "\n",
        "Each model has its own tokenizer to handle punctuation, etc. T**hat's why we need to import the correct tokenizer for the model of our choice.** Check out this well-written summary of tokenizers: https://huggingface.co/docs/transformers/tokenizer_summary\n",
        "\n",
        "**HuggingFace tokenizer** automatically downloads the vocabulary used during pretraining or fine-tuning a given model. We need not create our own vocab from the dataset for fine-tuning.  The **AutoTokenizer.from_pretrained** method takes in the name of the model to build the appropriate tokenizer."
      ],
      "metadata": {
        "id": "MuLM3vr5Bo4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Download and Prepare the Dataset\n",
        "\n",
        "In this tutorial, we're using the IMDB dataset."
      ],
      "metadata": {
        "id": "l7-74eoiCtsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget downloads it, !tar extracts it\n",
        "\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "CtnHV_FXBb6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might need to do some minor pre-processing like test-train splitting, separating text and labels, merging text, etc. In our case, the **read_imdb_split function** will split the text and the label."
      ],
      "metadata": {
        "id": "syCK79XeDBFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "afuuakCZGkr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# found this in another article https://huggingface.co/transformers/v4.11.3/custom_datasets.html\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "    split_dir = Path(split_dir)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_dir in [\"pos\", \"neg\"]:\n",
        "        for text_file in (split_dir/label_dir).iterdir():\n",
        "            texts.append(text_file.read_text())\n",
        "            labels.append(0 if label_dir == \"neg\" else 1)\n",
        "\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "OvtbrbiEECTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
        "test_texts, test_labels = read_imdb_split('aclImdb/test')\n",
        "\n",
        "# print(train_texts[:2])"
      ],
      "metadata": {
        "id": "jYTzNGGZC9m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will also create a train-validation split."
      ],
      "metadata": {
        "id": "zv0tzMxqFt-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using sklearn's tt split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts,\n",
        "                                                                  train_labels,\n",
        "                                                                  test_size=.2)\n"
      ],
      "metadata": {
        "id": "KPjpXcxDFEly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n",
        "\n",
        "The HuggingFace tokenizer will do the heavy lifting. We can either use AutoTokenizer which under the hood will call the correct tokenization class associated with the model name or we can directly import the tokenizer associated with the model (DistilBERT in our case). Also, note that the tokenizers are available in two flavors: a full python implementation and a “fast” implementation."
      ],
      "metadata": {
        "id": "SkqAMGv_G7h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DistilBertTokenizerFast"
      ],
      "metadata": {
        "id": "RTQhY94RHlQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HplADiAejbmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFDistilBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
        "outputs = model(inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "ig5WLJXBjxAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'"
      ],
      "metadata": {
        "id": "I9NhjObsHyd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pick one to use, either auto or specific\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "# tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "Uv_aDoh1HjKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will feed in the sentence (text) to the tokenizer which will return encoder text (tokens converted to ids).\n",
        "\n",
        "Learn about truncation and padding arguments here: https://huggingface.co/docs/transformers/preprocessing#everything-you-always-wanted-to-know-about-padding-and-truncation"
      ],
      "metadata": {
        "id": "JFvp4fzRIyNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "SNydiIDQIA8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a structured Dataset from a set of input encodings (train_encodings) and corresponding labels (train_labels).\n",
        "\n",
        "Create TF Dataset if you are using TensorFlow backend to fine-tune the HuggingFace transformer. In the case of PyTorch create PyTorch DataLoader."
      ],
      "metadata": {
        "id": "Zt_560N1KuON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hjJ0oPxmMNjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))"
      ],
      "metadata": {
        "id": "kkDwmPe9JefO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# added this later since a function needed it at the end\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "metadata": {
        "id": "673Vp_JXRaTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Transformer Models\n",
        "\n",
        "The HuggingFace Transformer models are compatible with native PyTorch and TensorFlow 2.x. Models are standard torch.nn.Module or tf.keras.Model depending on the prefix of the model class name. If it begins with TF then it's a tf.keras.Model. Note that tokenizers are framework agnostic.\n",
        "\n",
        "The easiest way to download a pre-trained Transformer model is to use the appropriate AutoModel(TFAutoModelForSequenceClassification in our case).\n",
        "\n",
        "You can find the list of pre-trained models here: https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "We will import TFDistilBertForSequenceClassification since we are fine-tuning a DistilBERT transformer. This will download the pre-trained model along with the classification head.\n"
      ],
      "metadata": {
        "id": "wieN1VyBLbz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required model class\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "# Download pre-trained model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "PeebT6yELIsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we need the output layer(head) to have 3 neurons, we can initialize the same by passing num_labels=3 to the model class. It will create a DistilBERT model (in our case) instance with encoder weights copied from the distilbert-base-uncased model and a randomly initialized sequence classification head on top of the encoder with an output size of 3."
      ],
      "metadata": {
        "id": "laSStD6ZMs0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME,\n",
        "                                                              num_labels=3)"
      ],
      "metadata": {
        "id": "NycNQ4rRLMQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also ask the model to return all hidden states and all attention weights if we need them"
      ],
      "metadata": {
        "id": "xEQQnh33M7wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME,\n",
        "                                              output_hidden_states=True,\n",
        "                                              output_attentions=True)"
      ],
      "metadata": {
        "id": "FYM5oT2NM3g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can change how the model itself is built, by defining custom configuration class.\n",
        "\n",
        "Each architecture comes with its own relevant configuration (in the case of DistilBERT, DistilBertConfig) which allows us to specify any of the hidden dimensions, dropout rate, etc.\n",
        "\n",
        "**However, by doing so we will have to train the model from scratch. (Not covered in this tutorial.)**"
      ],
      "metadata": {
        "id": "NixABtoOM_um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertConfig\n",
        "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
        "model = TFDistilBertForSequenceClassification(config)"
      ],
      "metadata": {
        "id": "jtCHoVfuNERt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required model class\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "\n",
        "# Download pre-trained model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "fgIk1Yi8l-3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning\n",
        "## Feature Complete Trainer / TFTrainer\n",
        "\n",
        "You can fine-tune a HuggingFace Transformer using both native PyTorch and TensorFlow 2.\n",
        "\n",
        "HuggingFace provides a simple but feature-complete training and evaluation interface through **Trainer()/TFTrainer().**\n",
        "\n",
        "We can train, fine-tune, and evaluate any HuggingFace Transformers model with a wide range of training options and with built-in features like metric logging, gradient accumulation, and mixed precision.\n"
      ],
      "metadata": {
        "id": "mlpqZsbhNTw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### But first, training arguments.\n",
        "\n",
        "Before instantiating Trainer/TFTrainer, we need to create a TrainingArguments/TFTrainingArguments **to access all the points of customization during training**."
      ],
      "metadata": {
        "id": "Uo_Q9JSSNteM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some notable arguments are:\n",
        "* per_device_train_batch_size: The batch size per GPU/TPU core/CPU for training.\n",
        "\n",
        "* gradient_accumulation_steps: Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "\n",
        "* learning_rate: The initial learning rate for Adam.\n",
        "\n",
        "* weight_decay: The weight decay to apply (if not zero).\n",
        "\n",
        "* num_train_epochs: Total number of training epochs to perform.\n",
        "\n",
        "* run_name:  A descriptor for the run used for Weights and Biases logging.\n",
        "\n",
        "Learn more about these args here: https://huggingface.co/transformers/main_classes/trainer.html?highlight=tftrainingarguments#tftrainingarguments\n",
        "\n",
        "If you are using PyTorch DataLoader then use TrainingArguments. You can learn more about the arguments here (https://huggingface.co/docs/transformers/main_classes/trainer?highlight=tftrainingarguments#trainingarguments) . Note that there are some additional features that you can use with TrainingArguments like early stopping and label smoothing.\n",
        "\n"
      ],
      "metadata": {
        "id": "KZw5S5DhOAI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFTrainer, TFTrainingArguments"
      ],
      "metadata": {
        "id": "43y5r1Z8Omdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mwdf6p3RO8Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=2,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "OH_xiBhDNQaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TFTrainer(\n",
        "    model=model,                     # the instantiated HF Transformers model to be trained\n",
        "    args=training_args,              # training arguments, defined above\n",
        "    train_dataset=train_dataset,     # training dataset\n",
        "    eval_dataset=val_dataset         # evaluation dataset\n",
        ")"
      ],
      "metadata": {
        "id": "PGkdSkGcO8-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "HBtRFKi8R8yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "7z5ZFqrPVnw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJhqas5fSDhY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}